var <- c("beta", "gamma", "theta", "alpha", "delta", "A", "Th", "rho_A", "sigma_A", "rho_Th", "sigma_Th", "K") #exogeneous parameters and stochastic variables
init <- c(0.99, 1, 3.48, 1/3, 0.025, 1, 1, 0.95, 0.007, 0.5, 0.01, 0) #assign intial values
#assign variables
x <- c(0, 0, 0) #empty vector for initial variables K_t, A_t, theta_t
for(i in 1:length(var))
{
assign(var[i],init[i])
}
#______ Setup Function ______####
Impuls.response <- function(parameter, type, length) #function of parameter, type and length (how many periods)
{
if(type == "IRF") #for the shock set up
{
eps <- parameter #definition of epsilon, the parameter value
#______ 1st Step
x_t <- H%*%x + eps #apply the matrix to K_t, A_t, theta_t (set p of stochastic process)
Impuls_response <- cbind(c(x_t,rep(0,length(name)-3))) #matrix of
#______ Subsequent Steps
for(i in 2:length)
{
y_t<-G%*%x_t
x_t<-H%*%x_t
Impuls_response<-cbind(Impuls_response,c(x_t,y_t)) #just iterative apllication of matrices where no shocks occur after period 1, last line defines a matrix
}
}
if(type == "simulate") #for the simulation set up
{
Impuls_response<-NULL #overwrite I guess?
x_t<-c(0,0,0) #again empty vector for initial variables K_t, A_t, theta_t
for(i in 1:length) #first step (period 1)
{
eps_t<-c(0,0,0)
if(parameter[1]!=0)
{
stop("capital is non-stochastic")
}
if(parameter[2]!=0)
{
eps_t[2]<-rnorm(1,0,sigma_A) #A_t is stochastic, random draw of shocks from normal distribution with zero mean
}
if(parameter[3]!=0)
{
eps_t[3]<-rnorm(1,0,sigma_Th) #theta_t is stochastic, random draw of shocks from normal distribution with zero mean
}
x_t<-H%*%x_t+eps_t
y_t<-G%*%x_t
Impuls_response<-cbind(Impuls_response,c(x_t,y_t)) #again apply matrices iteratively for subsequent steps
}
}
Impuls_response<-ts(t(Impuls_response)) #time series of the computed model ("t" just transposes the matrix)
colnames(Impuls_response)<-name #set column names
plot(Impuls_response)
res<-Impuls_response #define res as a new object
mat<-matrix(rep(NA, ncol(res)*nrow(res)),ncol = ncol(res), nrow = nrow(res)) #just define a new matrix of adequate size
for(i in 1:ncol(res))
{
for(j in 1:nrow(res))
{
mat[j,i]<-res[j,i] #this loop just assigns values to the matrix
}
}
mat<-xts(mat,order.by = seq(from = as.Date("0000-01-01",format = "%Y-%m-%d"), to = as.Date("0000-01-01")+nrow(mat)-1, by = "day"))
colnames(mat)<-name
dat<<-mat
plot.xts(mat, bg = "white",minor.ticks = T, main = type, grid.ticks.on = "day",grid2="black", grid.col = "snow2")
addLegend(legend.loc = "topright", legend.names = name, lty=c(1), bg = "white", bty = "") #treat the matrix as a time-series object and order by date (and some graphical daiials)
}
#______ Play :D _______####
Impuls.response(c(0,-0.01,0.0),type = "simulate", length = 20) #type takes "shock" or "simulate"
dat
apply(dat,2,sd)#standard deviation
apply(dat,2,sd)/sd(dat[,"Y_t"])#relative standard deviation
cor(dat)[,"Y_t"]
G
for(i in 2:length)
{
y_t<-G%*%x_t
x_t<-H%*%x_t
Impuls_response<-cbind(Impuls_response,c(x_t,y_t)) #just iterative apllication of matrices where no shocks occur after period 1, last line defines a matrix
}
y_t
x
x_t
#______ Data Preparation _______####
H <- matrix(c(0.9447, 0.1367, -0.1042,
0,      0.9500,  0,
0,      0,       0.5000), byrow = TRUE, ncol = 3, nrow = 3) #define matrix
G <- matrix(c(0.5262, 0.4427, -0.0796,
0.1134, 1.6356, -1.0498,
-0.3299, 0.9534, -1.5747,
-1.2126, 5.4672, -4.1662,
0.4433, 0.6822,  0.5249,
-0.8866, 1.6356, -1.0498), byrow = TRUE, ncol = 3, nrow = 6) #define matrix
name <- c("K_t", "A_t", "Th_t", "C_t", "Y_t", "N_t", "I_t", "W_t", "R_t") #endogeneous variables
var <- c("beta", "gamma", "theta", "alpha", "delta", "A", "Th", "rho_A", "sigma_A", "rho_Th", "sigma_Th", "K") #exogeneous parameters and stochastic variables
init <- c(0.99, 1, 3.48, 1/3, 0.025, 1, 1, 0.95, 0.007, 0.5, 0.01, 0) #assign intial values
#assign variables
x <- c(0, 0, 0) #empty vector for initial variables K_t, A_t, theta_t
for(i in 1:length(var))
{
assign(var[i],init[i])
}
#______ Setup Function ______####
Impuls.response <- function(parameter, type, length) #function of parameter, type and length (how many periods)
{
if(type == "IRF") #for the shock set up
{
eps <- parameter #definition of epsilon, the parameter value
#______ 1st Step
x_t <- H%*%x + eps #apply the matrix to K_t, A_t, theta_t (set p of stochastic process)
}
}}}
x_t
x_t
x = [1, 2]
x
abc
abc <- c(1, 2)
abc
rep(abc, 2)
ABC <- matrix(c(1:6), 2, 3)
ABC
ABCD <- matrix(c1:6, 2, 3, byrow=TRUE)
ABCD
ABCD <- matrix(c1:6, 2, 3, byrow=TRUE)
ABCDx
ABCD
ABCD
ABCD
ABCD
ABCD <- matrix(c(1:6), 3, 2, byrow=TRUE)
ABCD
ABC%*%ABCD
ABC %*% ABCD
#______ 1st Step
x_t <- H %*% x + eps #vector of state variables K_t, A_t, theta_t (set p of stochastic process) in step 1; shock period
Impulse_response <- cbind(c(x_t, rep(0, length(name)-3))) #matrix of IRs for state variables
#______ 1st Step
x_t <- H %*% x + eps #vector of state variables K_t, A_t, theta_t (set p of stochastic process) in step 1; shock period
Impulse_response <- cbind(c(x_t, rep(0, length(name)-3))) #matrix of IRs for state variables
#______ 1st Step
x_t <- H %*% x + eps #vector of state variables K_t, A_t, theta_t (set p of stochastic process) in step 1; shock period
Impulse_response <- cbind(c(x_t, rep(0, length(name)-3))) #matrix of IRs for state variables
H <- matrix(c(0.9447, 0.1367, -0.1042,
0,      0.9500,  0,
0,      0,       0.5000), byrow = TRUE, ncol = 3, nrow = 3)
H
x
IR <- cbind(c(x_t, rep(0, 3)))
IR
IR
x_t
x_t
D <- matrix(c(1:9, 3, 3, byrow=TRUE))
v <- c(1, 2, 3)
Dv <- D %*% v
D v Dv
D v Dv
D
D <- matrix(c(1:9), 3, 3, byrow=TRUE)
v <- c(1, 2, 3)
Dv <- D %*% v
D
v
Dv
vector_x_t <- H %*% vector_x
vector_x <- c(0, 0, 0)
vector_x_t <- H %*% vector_x
vector_x_t
IR <- cbind(c(x_t, rep(0, 3)))
IR <- cbind(c(vector_x_t, rep(0, 3)))
IR
N
intercept
set.seed(1234)
N         <- 1000                            #how many observations?
intercept
intercept <- matrix(1, nrow = N, ncol = 1)   #intercept
intercept
intercept
x1
x1        <- rnorm(N)                        #normally distributed regressor
x1
rnorm(1)
N         <- 1000                            #how many observations?
rnorm(1)
rnorm(1)
rnorm(1)
# let's set a seed such that the results are reproducible
set.seed(1234)
rnorm(1)
rnorm(1)
# let's set a seed such that the results are reproducible
set.seed(1234)
rnorm(1)
# let's set a seed such that the results are reproducible
set.seed(1234)
rnorm(1)
rnorm(1)
N         <- 1000                            #how many observations?
rnorm(1)
# let's set a seed such that the results are reproducible
set.seed(1234)
rnorm(1)
# let's set a seed such that the results are reproducible
set.seed(1234)
intercept <- matrix(1, nrow = N, ncol = 1)   #intercept
true.beta <- c(2, -1)                        #true coefficients
sig2.true <- 0.75                            #true error variance
errors    <- rnorm(N, 0, sqrt(sig2.true))    #error terms
errors
X         <- cbind(intercept, x1)            #explanatory matrix
X
y         <- X %*% true.beta + errors
y
# let's look at OLS estimates
# coefficient estimates
# (X'X)^(-1) X'y
beta.ols   <- solve(t(X) %*% X) %*% t(X) %*% y
beta.ols
beta.ols
# let's set a seed such that the results are reproducible
set.seed(1234)
N         <- 1000                            #how many observations?
intercept <- matrix(1, nrow = N, ncol = 1)   #intercept
x1        <- rnorm(N)                        #normally distributed regressor
true.beta <- c(2, -1)                        #true coefficients
sig2.true <- 0.75                            #true error variance
errors    <- rnorm(N, 0, sqrt(sig2.true))    #error terms
X         <- cbind(intercept, x1)            #explanatory matrix
y         <- X %*% true.beta + errors
# let's look at OLS estimates
# coefficient estimates
# (X'X)^(-1) X'y
beta.ols   <- solve(t(X) %*% X) %*% t(X) %*% y
beta.ols
# variance estimate
errors.ols <- y - X %*% beta.ols
sig2.ols   <- t(errors.ols) %*% errors.ols / (N-ncol(X))
sig2.ols
t(errors.ols) %*% errors.ols
(N-ncol(X))
reg <- lm(y~X-1)
summary(reg)
b0    <- matrix(0, nrow=P)  #prior mean of beta
B0    <- diag(100,  P)      #prior var-cov of beta
B0inv
b0    <- matrix(0, nrow=P)  #prior mean of beta
save <- 10000         #number of saved draws
nburn <- 10000         #number of burn ins
ntot  <- nsave + nburn #number of total iterations
P     <- ncol(X)       #number of explanatories
N     <- nrow(X)       #number of observations
# ------------------- #
# --- PRIOR SETUP --- #
# ------------------- #
# second, we have to specify our prior distributions
# today, we will use conjugate priors
# next session will be about using arbitrary prior distributions
# normal priors on the coefficients
b0    <- matrix(0, nrow=P)  #prior mean of beta
B0    <- diag(100,  P)      #prior var-cov of beta
B0i
nsave <- 10000         #number of saved draws
nburn <- 10000         #number of burn ins
ntot  <- nsave + nburn #number of total iterations
P     <- ncol(X)       #number of explanatories
N     <- nrow(X)       #number of observations
# ------------------- #
# --- PRIOR SETUP --- #
# ------------------- #
# second, we have to specify our prior distributions
# today, we will use conjugate priors
# next session will be about using arbitrary prior distributions
# normal priors on the coefficients
b0    <- matrix(0, nrow=P)  #prior mean of beta
B0    <- diag(100,  P)      #prior var-cov of beta
B0inv
nsave <- 10000         #number of saved draws
nburn <- 10000         #number of burn ins
ntot  <- nsave + nburn #number of total iterations
P     <- ncol(X)       #number of explanatories
N     <- nrow(X)       #number of observations
# ------------------- #
# --- PRIOR SETUP --- #
# ------------------- #
# second, we have to specify our prior distributions
# today, we will use conjugate priors
# next session will be about using arbitrary prior distributions
# normal priors on the coefficients
b0    <- matrix(0, nrow=P)  #prior mean of beta
B0    <- diag(100,  P)      #prior var-cov of beta
B0inv <- solve(B0)          #inverse of prior var-cov of beta
# how does this prior distribution look like?
plot(density(rnorm(20000, b0[1], B0[1,1])))
c0 <- 2 #prior shape of sigma2
C0 <- 1 #prior rate  of sigma2
# how does this prior distribution look like?
plot(density(1/rgamma(20000, c0, C0)))
sig2.draw <- 1
# set coefficients to 0
beta.draw <- matrix(0, P, 1)
beta.store <- matrix(NA, nsave, P)
sig2.store <- matrix(NA, nsave, 1)
XX <- t(X)%*%X
XY <- t(X)%*%y
# compute posterior quantities
Bn_inv <- B0inv + XX / sig2.draw
Bn_inv
Bn_inv <- B0inv + XX / sig2.draw
Bn     <- solve(Bn_inv)
bn     <- Bn %*% (B0inv %*% b0 + XY / sig2.draw)
bn
Bn
# draw from a multivariate normal distribution
# (there are several ways to do that, here I use a package for simplicity)
beta.draw <- mnormt::rmnorm(1, bn, Bn)
beta.draw
e  <- y - X %*% beta.draw
# sum of squared residuals (equivalent to sum((e)^2))
ee <- t(e) %*% e
# compute posterior quantities
ck <- c0 +  N/2
Ck <- C0 + ee/2
# sample sigma2 from inverse gamma posterior
sig2.draw <- 1/rgamma(1, ck, Ck)
plot(density(10000, ck, Ck))
# sample sigma2 from inverse gamma posterior
sig2.draw <- 1/rgamma(1, ck, Ck)
sig2.draw
Bn_inv <- B0inv + XX / sig2.draw
Bn     <- solve(Bn_inv)
bn     <- Bn %*% (B0inv %*% b0 + XY / sig2.draw)
# draw from a multivariate normal distribution
# (there are several ways to do that, here I use a package for simplicity)
beta.draw <- mnormt::rmnorm(1, bn, Bn)
# STEP 2: SAMPLE SIGMA2 GIVEN BETA & DATA
# compute errors
e  <- y - X %*% beta.draw
# sum of squared residuals (equivalent to sum((e)^2))
ee <- t(e) %*% e
# compute posterior quantities
ck <- c0 +  N/2
Ck <- C0 + ee/2
sig2.draw
sig2.draw
--------------------- #
# --- SIMULATE DATA --- #
# --------------------- #
# let's set a seed such that the results are reproducible
set.seed(1234)
N         <- 1000                            #how many observations?
intercept <- matrix(1, nrow = N, ncol = 1)   #intercept
x1        <- rnorm(N)                        #normally distributed regressor
true.beta <- c(2, -1)                        #true coefficients
sig2.true <- 0.75                            #true error variance
errors    <- rnorm(N, 0, sqrt(sig2.true))    #error terms
X         <- cbind(intercept, x1)            #explanatory matrix
# now simulate dependent variable y using beta, X and errors
y         <- X %*% true.beta + errors
# ---------------------------- #
# --- CHECK SIMULATED DATA --- # (important!)
# ---------------------------- #
# let's look at OLS estimates
# coefficient estimates
# (X'X)^(-1) X'y
beta.ols   <- solve(t(X) %*% X) %*% t(X) %*% y
# variance estimate
errors.ols <- y - X %*% beta.ols
sig2.ols   <- t(errors.ols) %*% errors.ols / (N-ncol(X))
# you can get the same via Rs linear regression function
# note the -1 is necessary as we already included an intercept in X
reg <- lm(y~X-1)
summary(reg)
# alright, our simulated data is looking good
# now, let's start to set up a Gibbs sampling algorithm
# --------------------------- #
# --- GIBBS SAMPLER SETUP --- #
# --------------------------- #
# first, we have to throw in some general information that the sampler will use
nsave <- 10000         #number of saved draws
nburn <- 10000         #number of burn ins
ntot  <- nsave + nburn #number of total iterations
P     <- ncol(X)       #number of explanatories
N     <- nrow(X)       #number of observations
# ------------------- #
# --- PRIOR SETUP --- #
# ------------------- #
# second, we have to specify our prior distributions
# today, we will use conjugate priors
# next session will be about using arbitrary prior distributions
# normal priors on the coefficients
b0    <- matrix(0, nrow=P)  #prior mean of beta
B0    <- diag(100,  P)      #prior var-cov of beta
B0inv <- solve(B0)          #inverse of prior var-cov of beta
# how does this prior distribution look like?
plot(density(rnorm(20000, b0[1], B0[1,1])))
# inverse gamma prior on the variance
c0 <- 2 #prior shape of sigma2
C0 <- 1 #prior rate  of sigma2
# how does this prior distribution look like?
plot(density(1/rgamma(20000, c0, C0)))
# ----------------------- #
# --- STARTING VALUES --- #
# ----------------------- #
# in a next step, we have to provide some starting values for the Gibbs sampler
# we will stay agnostic and just use some arbitrary numbers
# you could also try using the OLS estimates as starting values if you want ''better'' starting values
# mathematically, it should not matter which values you pick
# set variance to 1
sig2.draw <- 1
# set coefficients to 0
beta.draw <- matrix(0, P, 1)
# ----------------------- #
# --- STORAGE ----------- #
# ----------------------- #
# finally, we have to create some containers where we can store the draws from the posterior distributions
# in each iteration, we get one draw for beta and one for sigma2
# we save many of them to be able to look at the simulated posterior distributions afterwards
beta.store <- matrix(NA, nsave, P)
sig2.store <- matrix(NA, nsave, 1)
# ------------------------------ #
# --- GIBBS SAMPLING ALGORITHM - #
# ------------------------------ #
# alright, the preliminary setup is done!
# now we will start to look into the Gibbs sampling algorithm
# we use a loop to iteratively draw from p(beta|sigma2,y,X) and p(sigma2|beta,y,X)
# compute sufficient statistics
XX <- t(X)%*%X
XY <- t(X)%*%y
for (irep in 1:ntot){ # MCMC LOOP START
# STEP 1: SAMPLE BETA GIVEN SIGMA & DATA
# compute posterior quantities
Bn_inv <- B0inv + XX / sig2.draw
Bn     <- solve(Bn_inv)
bn     <- Bn %*% (B0inv %*% b0 + XY / sig2.draw)
# draw from a multivariate normal distribution
# (there are several ways to do that, here I use a package for simplicity)
beta.draw <- mnormt::rmnorm(1, bn, Bn)
# STEP 2: SAMPLE SIGMA2 GIVEN BETA & DATA
# compute errors
e  <- y - X %*% beta.draw
# sum of squared residuals (equivalent to sum((e)^2))
ee <- t(e) %*% e
# compute posterior quantities
ck <- c0 +  N/2
Ck <- C0 + ee/2
# sample sigma2 from inverse gamma posterior
sig2.draw <- 1/rgamma(1, ck, Ck)
# STEP 3: STORE POSTERIOR SAMPLES
#only after burn-in period!
if(irep > nburn){
beta.store[irep-nburn,] <- beta.draw
sig2.store[irep-nburn,] <- sig2.draw
}
# STEP 4: PROGRESS
# (in case you want to know how many iterations we already did)
if(irep%%50==0){print(irep)}
# depending on how fancy your algorithm is supposed to be, there are progress bars available in R:
# https://ryouready.wordpress.com/2009/03/16/r-monitor-function-progress-with-a-progress-bar/
} # MCMC LOOP END
sig2.store
#coefficient 1, true value and OLS estimate
plot.ts(beta.store[,1])
apply(beta.store, 2, mean) #how does that compare to ols estimates?
beta.ols
coronadata <- read.csv("~/Documents/1 WU/1 Courses 1.2/Microeconometrics/coronadata/coronadata.csv")
View(coronadata)
atan2(-sqrt(12)/2)
atan2(-sqrt(12)/2))
atan2(-sqrt(12)/2)
print(Hello World)
print("Hello World")
library(readxl)
library(rdd)
library(devtools)
install.packages("devtools")
install.packages("devtools")
library(devtools)
library(dplyr)
library(rdrobust)
library(stargazer)
library(rddtools)
install_github("bquast/rddtools")
library(rddtools)
setwd("~/Documents/Github/rp-ie/data")
QE <- read_excel("QE_HF.xlsx")
library(readxl)
QE_HF <- read_excel("QE_HF.xlsx")
View(QE_HF)
data <- rdd_data(QE_HF$`USD/EUR`, QE_HF$Obsv, cutpoint = 0)
logdata <-rdd_data(log(QE_HF$`USD/EUR`), QE_HF$Obsv, cutpoint = 0)
rdplot(data$y, data$x, p=1,
x.lim = c(-1100,1100),
y.lim = c(0.70,0.8),
x.lab="Minutes",
y.lab="USDEUR", title = "")
rdd_reg_lm(
logdata,
covariates = NULL,
order = 1,
bw = NULL,
slope = "same"
)
#Creating Table in Stargazer for Latex
stargazer(rdd_reg_lm(
logdata,
covariates = NULL,
order = 1,
bw = NULL,
slope = "same"
))
